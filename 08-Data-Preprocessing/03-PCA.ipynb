{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Principal Component Analysis (PCA)?\n",
    "\n",
    "PCA is a statistical technique that transforms a high-dimensional dataset into a lower-dimensional space by identifying the **principal components**—directions (linear combinations of features) that capture the maximum variance in the data. These components are orthogonal (uncorrelated) and ordered by the amount of variance they explain.\n",
    "\n",
    "- **Key Steps in PCA:**\n",
    "  1. **Standardize the Data:** Center the features (zero mean) and scale them (unit variance) since PCA is sensitive to feature scales.\n",
    "  2. **Compute Covariance Matrix:** Measure how features vary together.\n",
    "  3. **Eigenvalue Decomposition:** Find eigenvectors (principal components) and eigenvalues (variance explained by each component).\n",
    "  4. **Project Data:** Transform the original data onto the top `k` principal components, reducing dimensionality from `n` features to `k`.\n",
    "\n",
    "- **Purpose:**\n",
    "  - Reduce dimensionality to simplify models, decrease computation time, and mitigate overfitting.\n",
    "  - Remove noise and redundant information by focusing on the most significant variance.\n",
    "\n",
    "- **Trade-Off:** PCA discards some information (variance in lower components), which might affect accuracy if important patterns are lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data shape: (353, 10)\n",
      "Sample of training data:\n",
      "          age       sex       bmi        bp        s1        s2        s3  \\\n",
      "17   0.070769  0.050680  0.012117  0.056301  0.034206  0.049416 -0.039719   \n",
      "66  -0.009147  0.050680 -0.018062 -0.033213 -0.020832  0.012152 -0.072854   \n",
      "137  0.005383 -0.044642  0.049840  0.097615 -0.015328 -0.016345 -0.006584   \n",
      "245 -0.027310 -0.044642 -0.035307 -0.029770 -0.056607 -0.058620  0.030232   \n",
      "31  -0.023677 -0.044642 -0.065486 -0.081413 -0.038720 -0.053610  0.059685   \n",
      "\n",
      "           s4        s5        s6  \n",
      "17   0.034309  0.027364 -0.001078  \n",
      "66   0.071210  0.000272  0.019633  \n",
      "137 -0.002592  0.017036 -0.013504  \n",
      "245 -0.039493 -0.049872 -0.129483  \n",
      "31  -0.076395 -0.037129 -0.042499  \n"
     ]
    }
   ],
   "source": [
    "### Example Using Scikit-learn’s `diabetes` Dataset\n",
    "\"\"\" We’ll use the `diabetes` dataset (10 features, 442 samples) to apply Linear Regression before and after PCA, comparing MSE and R². \"\"\"\n",
    "\n",
    "#### Step 1: Load and Prepare the Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = diabetes.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Original training data shape:\", X_train.shape)\n",
    "print(\"Sample of training data:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance without PCA:\n",
      "Mean Squared Error: 2900.19\n"
     ]
    }
   ],
   "source": [
    "#### Step 2: Train Model Without PCA (Baseline)\n",
    "\n",
    "# Train Linear Regression without PCA\n",
    "lr_baseline = LinearRegression()\n",
    "lr_baseline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_baseline = lr_baseline.predict(X_test)\n",
    "mse_baseline = mean_squared_error(y_test, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test, y_pred_baseline)\n",
    "\n",
    "print(\"Performance without PCA:\")\n",
    "print(f\"Mean Squared Error: {mse_baseline:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after PCA: (353, 5)\n",
      "Explained variance ratio: [0.39839796 0.14697333 0.12552716 0.09964256 0.0650106 ]\n",
      "Cumulative explained variance: [0.39839796 0.54537129 0.67089845 0.77054101 0.8355516 ]\n"
     ]
    }
   ],
   "source": [
    "#### Step 3: Apply PCA\n",
    "\"\"\" We’ll standardize the data (required for PCA) and reduce it to, say, 5 components (half the original features), then retrain the model. \"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA (retain 5 components)\n",
    "pca = PCA(n_components=5)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test) \n",
    "\n",
    "\n",
    "print(\"Training data shape after PCA:\", X_train_pca.shape)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative explained variance:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of components :  5\n"
     ]
    }
   ],
   "source": [
    "print(\"# of components : \",  pca.n_components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance without PCA:\n",
      "Mean Squared Error: 2864.10\n"
     ]
    }
   ],
   "source": [
    "# Train Linear Regression with PCA\n",
    "lr_pca = LinearRegression()\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_pca = lr_pca.predict(X_test_pca)\n",
    "mse_pca = mean_squared_error(y_test, y_pred_pca)\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(\"Performance without PCA:\")\n",
    "print(f\"Mean Squared Error: {mse_pca:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nutanix_ml_kernel",
   "language": "python",
   "name": "nutanix_ml_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
